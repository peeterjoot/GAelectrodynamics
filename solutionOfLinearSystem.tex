%
% Copyright Â© 2017 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
\index{linear system}
\index{wedge product!linear solution}
%\maketheorem{Exact solution of a linear system.}{thm:solutionOfLinearSystem:1}{
%Given \( n \) linearly independent vectors \( \Ba_1, \Ba_2, \cdots \Ba_n \), the system
%\begin{equation*}
%\Ba_1 x_1 + \Ba_2 x_2 \cdots + \Ba_n x_n = \Bb,
%\end{equation*}
%if it has a solution, is given by
%\begin{equation*}
%\begin{aligned}
%x_1 &= i^{-1} \lr{ \Bb \wedge \Ba_2 \wedge \cdots \wedge \Ba_n } \\
%x_2 &= i^{-1} \lr{ \Ba_1 \wedge \Bb \wedge \cdots \wedge \Ba_n } \\
%    & \vdots \\
%x_n &= i^{-1} \lr{ \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Bb },
%\end{aligned}
%\end{equation*}
%where
%\begin{equation*}
%i = \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Ba_n,
%\end{equation*}
%is the pseudoscalar for the hypervolume spanned by \( \setlr{ \Ba_1, \cdots, \Ba_n } \).
%If the dimension of the vectors \( \Ba_i \) is \( n \), then this solution is equivalent to Cramer's rule.
%} % theorem
\maketheorem{Best fit solution of linear system.}{thm:solutionOfLinearSystem:2}{
Given \( k \) linearly independent vectors \( \Ba_1, \Ba_2, \cdots \Ba_k \), and the projection \( \Bb_\parallel \)
of a vector \( \Bb \) onto the hypervolume spanned by \( \setlr{ \Ba_1, \cdots, \Ba_k } \)
\begin{equation*}
\Bb_\parallel = i^{-1} \lr{ i \cdot \Bb },
\end{equation*}
where \( i = \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Ba_k \), is a pseudoscalar for that hypervolume, then the system
\begin{equation*}
\Ba_1 x_1 + \Ba_2 x_2 \cdots + \Ba_k x_k = \Bb_\parallel,
\end{equation*}
is solved by
\begin{equation*}
\begin{aligned}
x_1 &= i^{-1} \cdot \lr{ \Bb \wedge \Ba_2 \wedge \cdots \wedge \Ba_k } \\
x_2 &= i^{-1} \cdot \lr{ \Ba_1 \wedge \Bb \wedge \cdots \wedge \Ba_k } \\
    & \vdots \\
x_n &= i^{-1} \cdot \lr{ \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Bb }.
\end{aligned}
\end{equation*}

If \( \Bb \in \Span \setlr{ \Ba_1, \cdots, \Ba_k } \), so that \( \Bb_\parallel = \Bb \), then the dot products between the k-blades above may be dropped.
} % theorem
This is equivalent to a Moore-Penrose or SVD pseudoinverse solution for the system
\begin{equation}\label{eqn:solutionOfLinearSystem:101}
\begin{bmatrix}
\Ba_1 & \cdots & \Ba_n
\end{bmatrix}
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}
=
\Bb.
\end{equation}
Furthermore, also when the system is exact,
if the dimension of the vectors \( \Ba_i \) is \( k \), then this solution is equivalent to Cramer's rule.

Rather than formally trying to prove this theorem, we can tackle it informally, starting with some examples.
\subsubsection{Example: two variable system.}
The simplest example is that of a two variable system
\begin{equation}\label{eqn:solutionOfLinearSystem:1160}
\Ba x + \Bb y = \Bc.
\end{equation}
Let's proceed to solve this using the wedge product, assuming to start with that the system has an exact solution (i.e.: that \( \Bc \) is a linear combination of \( \Ba, \Bb \).)

To solve for \( x \) simply wedge with \( \Bb \), and to solve for \( y \) wedge with \( \Ba \)
\begin{equation}\label{eqn:solutionOfLinearSystem:1180}
\begin{aligned}
\lr{ \Ba x + \cancel{\Bb} y } \wedge \Bb &= \Bc \wedge \Bb \\
\Ba \wedge \lr{ \cancel{\Ba} x + \Bb y } &= \Ba \wedge \Bc,
\end{aligned}
\end{equation}
so, the solution, if it exists, is given by
\begin{equation}\label{eqn:solutionOfLinearSystem:1200}
\begin{aligned}
x &= \inv{ \Ba \wedge \Bb } \Bc \wedge \Bb \\
y &= \inv{ \Ba \wedge \Bb } \Ba \wedge \Bc.
\end{aligned}
\end{equation}
\subsubsection{Example: exact \( k \) variable system.}
This idea generalizes trivially to
higher order systems can be solved, simply requiring wedging more times to eliminate all terms other than the one of interest.

For example, if the \( k \) variable system
\begin{equation}\label{eqn:solutionOfLinearSystem:1220}
\Ba_1 x_1 + \Ba_2 x_2 \cdots + \Ba_k x_k = \Bb,
\end{equation}
has a solution, we can solve for any of the \( x_i \)'s by wedging repeatedly.  For example, we can find \( x_1 \)
by wedging with all \( \Ba_2, \cdots \Ba_k \), to find
\begin{equation}\label{eqn:solutionOfLinearSystem:102}
x_1 \lr{ \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Ba_k } = \Bb \wedge \Ba_2 \wedge \cdots \wedge \Ba_k,
\end{equation}
or
\begin{equation}\label{eqn:solutionOfLinearSystem:103}
x_1 = \inv{ \Ba_1 \wedge \Ba_2 \wedge \cdots \wedge \Ba_k } \lr{ \Bb \wedge \Ba_2 \wedge \cdots \wedge \Ba_k }
\end{equation}
If this system has no solution, then these k-vector ratios will not be scalars.

It's fairly easy to see that to solve for \( x_j \), we start switch the numerator to the pseudoscalar \( i \), with \( \Bb \) taking the place of \( \Ba_j \).
\subsubsection{Example: \R{3} Cramer's rule.}
If this sounds like Cramer's rule, that is because the two are equivalent when the dimension of the vector equals the number of variables in the linear system.
\index{Cramer's rule}
For example, consider the solution for \( x_1 \) of \cref{eqn:solutionOfLinearSystem:1220} for an \R{3} system, with \( \Ba_1 = \Bu, \Ba_2 = \Bv, \Ba_3 = \Bw \)
\begin{equation}\label{eqn:solutionOfLinearSystem:1260}
x_1 =
\frac{ \Bb \wedge \Bv \wedge \Bw }
{ \Bu \wedge \Bv \wedge \Bw }
=
\frac{
\begin{vmatrix}
b_1 & v_1 & w_1 \\
b_2 & v_2 & w_2 \\
b_3 & v_3 & w_3 \\
\end{vmatrix}
\cancel{\Be_1 \Be_2 \Be_3}
}
{
\begin{vmatrix}
u_1 & v_1 & w_1 \\
u_2 & v_2 & w_2 \\
u_3 & v_3 & w_3 \\
\end{vmatrix}
\cancel{\Be_1 \Be_2 \Be_3}
},
\end{equation}
which is exactly the ratio of determinants found in the Cramer's rule solution of this problem.  We get Cramer's rule for free due to the antisymmetric structure of the wedge product.

Cramer's rule doesn't apply to cases where the dimension of the space exceeds the number of variables, but a wedge product solution does not have that restriction.
\subsubsection{Example: Some \R{4} vectors.}
As an example, consider the two variable system \cref{eqn:solutionOfLinearSystem:1160} for vectors in \R{4} as follows
\begin{equation}\label{eqn:solutionOfLinearSystem:1280}
\Ba =
\begin{bmatrix}
1 \\
1 \\
0 \\
0
\end{bmatrix}, \qquad
\Bb =
\begin{bmatrix}
1 \\
0 \\
0 \\
1
\end{bmatrix}, \qquad
\Bc =
\begin{bmatrix}
1 \\
2 \\
0 \\
-1
\end{bmatrix}.
\end{equation}

%\iftoggle{kindle-version}{}{
Here's a (Mathematica) computation of the wedge products for the solution
%\href{https://github.com/jlaragonvera/Geometric-Algebra}{CliffordBasic.m}
\footnote{%
Using the CliffordBasic.m geometric algebra module from \citep{jlaragonveraGeometricAlgebra}.%
}

\begin{mmaCell}[moredefined={a, b, c, iab, aWedgeB, cWedgeB, aWedgeC, x, y, e, OuterProduct, GeometricProduct}]{Input}
  ClearAll[a, b, c, iab, aWedgeB, cWedgeB, aWedgeC, x, y]
  a = e[1] + e[2];
  b = e[1] + e[4];
  c = e[1] + 2 e[2] - e[4];

  aWedgeB = OuterProduct[a, b];
  cWedgeB = OuterProduct[c, b];
  aWedgeC = OuterProduct[a, c];

  (* 1/aWedgeB *)
  iab = aWedgeB / GeometricProduct[aWedgeB, aWedgeB];
  x = GeometricProduct[iab, cWedgeB];
  y = GeometricProduct[iab, aWedgeC];

  \{\{a \(\pmb{\wedge}\) b = , aWedgeB\},\{c \(\pmb{\wedge}\) b = , cWedgeB\},
  \{a \(\pmb{\wedge}\) c = , aWedgeC\},\{\"x = \", x\},\{\"y = \", y\}
  \} // Grid
\end{mmaCell}
\begin{mmaCell}{Output}
  a \(\wedge\) b = 	-e[1,2] + e[1,4] + e[2,4]
  c \(\wedge\) b = 	-2 e[1,2] + 2 e[1,4] + 2 e[2,4]
  a \(\wedge\) c = 	e[1,2] - e[1,4] - e[2,4]
  x = 	2
  y = 	-1
\end{mmaCell}

which shows that \( 2 \Ba - \Bb = \Bc \).
%}
%%Which gives
%%\begin{equation}\label{eqn:solutionOfLinearSystem:1300}
%%\begin{aligned}
%%\Ba \wedge \Bb &= \Be_{21} + \Be_{14} + \Be_{24} \\
%%\Bc \wedge \Bb &= 2 \Be_{21} + 2 \Be_{14} + 2 \Be_{24} \\
%%\Ba \wedge \Bc &= -\Be_{21} - \Be_{14} - \Be_{24} \\
%%x &= 2 \\
%%y &= -1.
%%\end{aligned}
%%\end{equation}
%%
\subsubsection{Example: intersection of two lines.}
As a concrete example, let's solve the intersection of two lines problem illustrated in \cref{fig:intersectionOfLines:intersectionOfLinesFig1}.
\pmathImageFigure{../figures/GAelectrodynamics/\subfigdir/}{intersectionOfLinesFig1}{Intersection of two lines.}{fig:intersectionOfLines:intersectionOfLinesFig1}{0.2}{orientedAreas.nb}

In parametric form, the lines in this problem are
\begin{equation}\label{eqn:solutionOfLinearSystem:1000}
\begin{aligned}
\Br_1(s) &= \Ba_0 + s( \Ba_1 - \Ba_0 ) \\
\Br_2(t) &= \Bb_0 + t( \Bb_1 - \Bb_0 ),
\end{aligned}
\end{equation}
so the solution, if it exists, is found at the point satisfying the equality
\begin{equation}\label{eqn:solutionOfLinearSystem:1020}
\Ba_0 + s( \Ba_1 - \Ba_0 ) = \Bb_0 + t( \Bb_1 - \Bb_0 ).
\end{equation}

With
\begin{equation}\label{eqn:solutionOfLinearSystem:1040}
\begin{aligned}
\Bu_1 &= \Ba_1 - \Ba_0 \\
\Bu_2 &= \Bb_1 - \Bb_0 \\
\Bd &= \Ba_0 - \Bb_0,
\end{aligned}
\end{equation}
the desired equation to solve is
\begin{equation}\label{eqn:solutionOfLinearSystem:1060}
\Bd + s \Bu_1 = t \Bu_2.
\end{equation}

As with any linear system, we can
solve for \( s \) or \( t \) by
wedging both sides with one of \( \Bu_1 \) or \( \Bu_2 \)
\begin{equation}\label{eqn:solutionOfLinearSystem:1080}
\begin{aligned}
\Bd \wedge \Bu_1 &= t \Bu_2 \wedge \Bu_1 \\
\Bd \wedge \Bu_2 + s \Bu_1 \wedge \Bu_2 &= 0.
\end{aligned}
\end{equation}

In \R{2} these equations have a solution if \( \Bu_1 \wedge \Bu_2 \ne 0 \), and in \R{N} these have solutions if the bivectors on each sides of the equations describe the same plane (i.e. the bivectors on each side of \cref{eqn:solutionOfLinearSystem:1080} are related by a scalar factor).
Put another way, these have solutions when \( s \) and \( t \) are scalars with the values
\begin{equation}\label{eqn:solutionOfLinearSystem:1100}
\begin{aligned}
s &= \frac{\Bu_2 \wedge \Bd}{\Bu_1 \wedge \Bu_2} \\
t &= \frac{\Bu_1 \wedge \Bd}{\Bu_1 \wedge \Bu_2}.
\end{aligned}
\end{equation}

%%%In
%%%\R{2}
%%%with
%%%\begin{equation}\label{eqn:solutionOfLinearSystem:1120}
%%%\begin{aligned}
%%%\Bu_1 &= u_{11} \Be_1 + u_{12} \Be_2 \\
%%%\Bu_2 &= u_{21} \Be_1 + u_{22} \Be_2 \\
%%%\Bd &= d_{1} \Be_1 + d_{2} \Be_2,
%%%\end{aligned}
%%%\end{equation}
%%%
%%%the wedge products in \cref{eqn:solutionOfLinearSystem:1100}
%%%can be expressed explicitly as a (unit bivector scaled) determinants
%%%
%%%\begin{equation}\label{eqn:solutionOfLinearSystem:1140}
%%%%\begin{aligned}
%%%s =
%%%\frac{
%%%\begin{vmatrix}
%%%u_{21} & u_{22} \\
%%%d_1 & d_2
%%%\end{vmatrix}
%%%\Be_{12}
%%%}
%%%{
%%%\begin{vmatrix}
%%%u_{11} & u_{12} \\
%%%u_{21} & u_{22} \\
%%%\end{vmatrix}
%%%\Be_{12}
%%%}
%%%%=
%%%%\frac{
%%%%\begin{vmatrix}
%%%%u_{21} & u_{22} \\
%%%%d_1 & d_2
%%%%\end{vmatrix}
%%%%}
%%%%{
%%%%\begin{vmatrix}
%%%%u_{11} & u_{12} \\
%%%%u_{21} & u_{22} \\
%%%%\end{vmatrix}
%%%%}
%%%\qquad
%%%t =
%%%\frac{
%%%\begin{vmatrix}
%%%u_{11} & u_{12} \\
%%%d_1 & d_2
%%%\end{vmatrix}
%%%\Be_{12}
%%%}
%%%{
%%%\begin{vmatrix}
%%%u_{11} & u_{12} \\
%%%u_{21} & u_{22} \\
%%%\end{vmatrix}
%%%\Be_{12}
%%%}
%%%%=
%%%%\frac{
%%%%\begin{vmatrix}
%%%%u_{11} & u_{12} \\
%%%%d_1 & d_2
%%%%\end{vmatrix}
%%%%}
%%%%{
%%%%\begin{vmatrix}
%%%%u_{11} & u_{12} \\
%%%%u_{21} & u_{22} \\
%%%%\end{vmatrix}
%%%%}
%%%.
%%%%\end{aligned}
%%%\end{equation}
%%%
%%%Once the unit bivectors \( \Be_{12} \) are cancelled \cref{eqn:solutionOfLinearSystem:1140} is the Cramer's rule solution of the problem.  Cramer's rule is seen to follow directly from the use of the wedge product to eliminate factors that are not of interest.
%%%In a similar way, the use of the wedge product for a 3D intersection problem with three variables, will lead directly to the Cramer's rule solution.
%%%
\makeproblem{Intersection of a line and plane.}{problem:solutionOfLinearSystem:1}{
Let a line be parameterized by
\begin{equation*}
\Br(a) = \Bp + \alpha \Ba,
\end{equation*}
and a plane be parameterized by
\begin{equation*}
\Br(b,c) = \Bq + \beta \Bb + \gamma \Bc.
\end{equation*}
\makesubproblem{}{problem:solutionOfLinearSystem:1:a}
For the intersection of the two, state the vector equation to be solved, and its solution for \( a \) in terms of a ratio of wedge products.
\makesubproblem{}{problem:solutionOfLinearSystem:1:b}
State the conditions for which the solution exist in \R{3} and \R{N}.
\makesubproblem{}{problem:solutionOfLinearSystem:1:c}
In terms of coordinates in \R{3} write out the ratio of wedge products as determinants and compare to the Cramer's rule solution.
} % problem
\makeanswer{problem:solutionOfLinearSystem:1}{
\makesubanswer{}{problem:solutionOfLinearSystem:1:a}
We are looking for solutions \( \alpha, \beta, \gamma \) such that the equality
\begin{equation}\label{eqn:solutionOfLinearSystem:1320}
\Bp + \alpha \Ba = \Bq + \beta \Bb + \gamma \Bc,
\end{equation}
is satisfied.
We have only to wedge with \( \Bb \wedge \Bc \), to find
\begin{equation}\label{eqn:solutionOfLinearSystem:1340}
\Bp \wedge \Bb \wedge \Bc + \alpha \lr{ \Ba \wedge \Bb \wedge \Bc }  = \Bq \wedge \Bb \wedge \Bc,
\end{equation}
or
\begin{equation}\label{eqn:solutionOfLinearSystem:1360}
\alpha = \frac{\lr{ \Bq - \Bp } \wedge \Bb \wedge \Bc }{ \Ba \wedge \Bb \wedge \Bc }.
\end{equation}
\makesubanswer{}{problem:solutionOfLinearSystem:1:b}
For \R{3}, a solution exists provided \( \Ba \wedge \Bb \wedge \Bc \ne 0 \), but for \R{N} a solution also requires
\begin{equation}\label{eqn:solutionOfLinearSystem:1380}
\lr{ \Bq - \Bp } \wedge \Bb \wedge \Bc \propto \Ba \wedge \Bb \wedge \Bc.
\end{equation}
For instance, there is no solution if \( \lr{ \Bq - \Bp } \wedge \Bb \wedge \Bc = \Be_{124} \), but \( \Ba \wedge \Bb \wedge \Bc = \Be_{234} \).
\makesubanswer{}{problem:solutionOfLinearSystem:1:c}
To solve this equation using coordinates, we seek solutions to
\begin{equation}\label{eqn:solutionOfLinearSystem:1400}
\Bp - \Bq = -\alpha \Ba + \beta \Bb + \gamma \Bc,
\end{equation}
or
\begin{equation}\label{eqn:solutionOfLinearSystem:1420}
\lr{ \Bp - \Bq } \cdot \Be_k = \lr{ -\alpha \Ba + \beta \Bb + \gamma \Bc } \cdot \Be_k,
\end{equation}
\( \forall k \in [1, N] \).  In matrix form, this is
\begin{equation}\label{eqn:solutionOfLinearSystem:1440}
\begin{bmatrix}
p_1 - q_1 \\
p_2 - q_2 \\
\vdots \\
p_N - q_N \\
\end{bmatrix}
=
\begin{bmatrix}
-a_1 & b_1 & c_1 \\
-a_2 & b_2 & c_2 \\
     & \vdots & \\
-a_N & b_N & c_N \\
\end{bmatrix}
\begin{bmatrix}
\alpha \\
\beta \\
\gamma
\end{bmatrix}.
\end{equation}
The Cramer's rule solution only applies to the \R{3} system, and has the form
\begin{equation}\label{eqn:solutionOfLinearSystem:1460}
\begin{bmatrix}
\alpha \\
\beta \\
\gamma
\end{bmatrix}
=
\frac{
\begin{vmatrix}
p_1 - q_1 & b_1 & c_1 \\
p_2 - q_2 & b_2 & c_2 \\
p_3 - q_3 & b_3 & c_3 \\
\end{vmatrix}
}
{
\begin{vmatrix}
-a_1 & b_1 & c_1 \\
-a_2 & b_2 & c_2 \\
-a_3 & b_3 & c_3 \\
\end{vmatrix}
}
=
\frac{
\begin{vmatrix}
q_1 - p_1 & b_1 & c_1 \\
q_2 - p_2 & b_2 & c_2 \\
q_3 - p_3 & b_3 & c_3 \\
\end{vmatrix}
}
{
\begin{vmatrix}
a_1 & b_1 & c_1 \\
a_2 & b_2 & c_2 \\
a_3 & b_3 & c_3 \\
\end{vmatrix}
}.
\end{equation}
This is obviously equivalent to the GA solution, where the ratio of determinants is found immediately from the coordinate representation of a triple wedge product.  We can't solve this system of equations using Cramer's rule for \R{N} when \( N > 3 \) since the system is overspecified in that case.  That overspecification is why we require the additional
\( \lr{ \Bq - \Bp } \wedge \Bb \wedge \Bc \propto \Ba \wedge \Bb \wedge \Bc \)
constraint for the GA solution using wedge products.  Note that this wedge product solution method is unlikely to be numerically stable for \( N > 3 \), and we are probably better off solving with SVD, so that we have some estimation of the numerical errors that either rule out or validate the solution.
} % answer
\subsubsection{Example: Best fit solution for two variable system.}
Now, let's consider the case where the system cannot be solved exactly.  It's sufficient to illustrate the ideas using just two variables.

Geometrically, the best we can do is to try to solve the related ``least squares'' problem
\begin{equation}\label{eqn:solutionOfLinearSystem:1480}
x \Ba + y \Bb = \Bc_\parallel,
\end{equation}
where \( \Bc_\parallel \) is the projection of \( \Bc \) onto the plane spanned by \( \Ba, \Bb \).  Regardless of the value of \( \Bc \), we can always find a solution to this problem.  For example, solving for \( x \), we have
\begin{equation}\label{eqn:solutionOfLinearSystem:1500}
\begin{aligned}
x
&= \inv{ \Ba \wedge \Bb } \Bc_\parallel \wedge \Bb \\
&= \inv{ \Ba \wedge \Bb } \cdot \lr{ \Bc_\parallel \wedge \Bb } \\
&= \inv{ \Ba \wedge \Bb } \cdot \lr{ \Bc \wedge \Bb } - \inv{ \Ba \wedge \Bb } \cdot \lr{ \Bc_\perp \wedge \Bb }.
\end{aligned}
\end{equation}
The zero above follows because \( \Bc_\perp \) is perpendicular to both \( \Ba \) and \( \Bb \) by construction.  Geometrically, we are trying to dot two perpendicular bivectors, where \( \Bb \) is a common factor of those two bivectors, as illustrated in \cref{fig:perpPlanes:perpPlanesFig1}.
\imageFigure{../figures/blogit/perpPlanesFig1}{Perpendicular bivectors.}{fig:perpPlanes:perpPlanesFig1}{0.4}

We see that the solution to this two variable linear system problem, is
\begin{subequations}
\label{eqn:solutionOfLinearSystem:1541}
\begin{equation}\label{eqn:solutionOfLinearSystem:1540}
x = \inv{ \Ba \wedge \Bb } \cdot \lr{ \Bc \wedge \Bb }.
\end{equation}
\begin{equation}\label{eqn:solutionOfLinearSystem:1560}
y = \inv{ \Ba \wedge \Bb } \cdot \lr{ \Ba \wedge \Bc }.
\end{equation}
\end{subequations}
\makeproblem{Perpendicular blades.}{problem:solutionOfLinearSystem:2}{
Show algebraically, that the second term from \cref{eqn:solutionOfLinearSystem:1500}
\begin{equation*}
- \inv{ \Ba \wedge \Bb } \cdot \lr{ \Bc_\perp \wedge \Bb },
\end{equation*}
is zero.
} % problem
\makeanswer{problem:solutionOfLinearSystem:2}{
We can reduce that second term, first expanding the bivector inverse explicitly
\begin{equation}\label{eqn:solutionOfLinearSystem:1520}
- \inv{ \Ba \wedge \Bb } \cdot \lr{ \Bc_\perp \wedge \Bb }
=
- \frac{ \Ba \wedge \Bb }{ \lr{ \Ba \wedge \Bb}^2 } \cdot \lr{ \Bc_\perp \wedge \Bb }.
\end{equation}
We can ignore the scalar \( -1/\lr{ \Ba \wedge \Bb}^2 \) factor, and expand the bivector dot product, to find
\begin{equation}\label{eqn:solutionOfLinearSystem:1580}
\begin{aligned}
\lr{ \Ba \wedge \Bb } \cdot \lr{ \Bc_\perp \wedge \Bb }
&=
\lr{ \lr{ \Ba \wedge \Bb } \cdot \Bc_\perp } \cdot \Bb \\
&=
\lr{ \Ba \lr{ \Bb \cdot \Bc_\perp} - \Bb \lr{ \Ba \cdot \Bc_\perp} } \cdot \Bb \\
&=
0.
\end{aligned}
\end{equation}
} % answer
\makeproblem{Two variable least squares problem.}{problem:solutionOfLinearSystem:3}{
We called the projection solution, a least-squares solution, without full justification.  Justify this by
finding the best fit solution to the two variable system
\begin{equation*}
x \Ba + y \Bb = \Bc,
\end{equation*}
by minimizing the squared error function
\begin{equation}\label{eqn:solutionOfLinearSystem:1600}
\epsilon = \lr{ \Bc - x \Ba - y \Bb }^2.
\end{equation}
Show that the resulting solution is identical to \cref{eqn:solutionOfLinearSystem:1541}.
} % problem
\makeanswer{problem:solutionOfLinearSystem:3}{
We follow the usual procedure, by equating all partials to zero
\begin{equation}\label{eqn:solutionOfLinearSystem:1620}
\begin{aligned}
0 &= \PD{x}{\epsilon} = 2 \lr{ \Bc - x \Ba - y \Bb } \cdot (-\Ba) \\
0 &= \PD{y}{\epsilon} = 2 \lr{ \Bc - x \Ba - y \Bb } \cdot (-\Bb).
\end{aligned}
\end{equation}
This is a two equation, two unknown system, which can be expressed in matrix form as
\begin{equation}\label{eqn:solutionOfLinearSystem:1640}
\begin{bmatrix}
\Ba^2 & \Ba \cdot \Bb \\
\Ba \cdot \Bb & \Bb^2
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
\begin{bmatrix}
\Ba \cdot \Bc \\
\Bb \cdot \Bc \\
\end{bmatrix}.
\end{equation}
This has solution
\begin{equation}\label{eqn:solutionOfLinearSystem:1660}
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
\inv{
\begin{vmatrix}
\Ba^2 & \Ba \cdot \Bb \\
\Ba \cdot \Bb & \Bb^2
\end{vmatrix}
}
\begin{bmatrix}
\Bb^2 & -\Ba \cdot \Bb \\
-\Ba \cdot \Bb & \Ba^2
\end{bmatrix}
\begin{bmatrix}
\Ba \cdot \Bc \\
\Bb \cdot \Bc \\
\end{bmatrix}
=
\frac{
\begin{bmatrix}
\Bb^2 \lr{ \Ba \cdot \Bc } - \lr{ \Ba \cdot \Bb} \lr{ \Bb \cdot \Bc } \\
\Ba^2 \lr{ \Bb \cdot \Bc } - \lr{ \Ba \cdot \Bb} \lr{ \Ba \cdot \Bc } \\
\end{bmatrix}
}{
\Ba^2 \Bb^2 - \lr{ \Ba \cdot \Bb }^2
}.
\end{equation}

All of these differences can be expressed as wedge dot products, using the following expansions in reverse
\begin{equation}\label{eqn:solutionOfLinearSystem:1680}
\begin{aligned}
\lr{ \Ba \wedge \Bb } \cdot \lr{ \Bc \wedge \Bd }
&=
\Ba \cdot \lr{ \Bb \cdot \lr{ \Bc \wedge \Bd } } \\
&=
\Ba \cdot \lr{ \lr{\Bb \cdot \Bc} \Bd - \lr{\Bb \cdot \Bd} \Bc } \\
&=
\lr{ \Ba \cdot \Bd } \lr{\Bb \cdot \Bc} - \lr{ \Ba \cdot \Bc }\lr{\Bb \cdot \Bd}.
\end{aligned}
\end{equation}

We find
\begin{equation}\label{eqn:solutionOfLinearSystem:1700}
\begin{aligned}
x
&= \frac{\Bb^2 \lr{ \Ba \cdot \Bc } - \lr{ \Ba \cdot \Bb} \lr{ \Bb \cdot \Bc }}{-\lr{ \Ba \wedge \Bb }^2 } \\
&= \frac{\lr{ \Ba \wedge \Bb } \cdot \lr{ \Bb \wedge \Bc }}{ -\lr{ \Ba \wedge \Bb }^2 } \\
&= \inv{ \Ba \wedge \Bb } \cdot \lr{ \Bc \wedge \Bb },
\end{aligned}
\end{equation}
and
\begin{equation}\label{eqn:solutionOfLinearSystem:1720}
\begin{aligned}
y
&= \frac{\Ba^2 \lr{ \Bb \cdot \Bc } - \lr{ \Ba \cdot \Bb} \lr{ \Ba \cdot \Bc } }{-\lr{ \Ba \wedge \Bb }^2 } \\
&= \frac{- \lr{ \Ba \wedge \Bb } \cdot \lr{ \Ba \wedge \Bc } }{ -\lr{ \Ba \wedge \Bb }^2 } \\
&= \inv{ \Ba \wedge \Bb } \cdot \lr{ \Ba \wedge \Bc }.
\end{aligned}
\end{equation}
Sure enough, we find what was dubbed the least squares solution, which we now know can be written out as a ratio of (dotted) wedge products.
%From \cref{eqn:solutionOfLinearSystem:1740}
} % answer
